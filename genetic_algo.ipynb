{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef56d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting deap\n",
      "  Downloading deap-1.4.1.tar.gz (1.1 MB)\n",
      "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 0.1/1.1 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 0.3/1.1 MB 4.1 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.4/1.1 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 0.8/1.1 MB 4.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 1.0/1.1 MB 4.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.1/1.1 MB 4.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from deap) (1.24.3)\n",
      "Building wheels for collected packages: deap\n",
      "  Building wheel for deap (setup.py): started\n",
      "  Building wheel for deap (setup.py): finished with status 'done'\n",
      "  Created wheel for deap: filename=deap-1.4.1-py3-none-any.whl size=97349 sha256=138f99c6a7e2b8f6d13f1aef79a821d0bfb2b3cc1936daffcfdea5f7131f83ae\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\f8\\64\\b8\\65eacfbff3024ae2e2beb22e691d5c8abb89fbd863b8049b5f\n",
      "Successfully built deap\n",
      "Installing collected packages: deap\n",
      "Successfully installed deap-1.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install deap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7f51a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4424f17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from deap import base, creator, tools, algorithms\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# Split the dataset into features (X) and the target variable (y)\n",
    "X = df.drop('Class', axis=1).values  # Assuming 'Class' is the target variable\n",
    "y = df['Class'].values\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the evaluation function (fitness function) using XGBoost\n",
    "def evaluate(individual, X, y):\n",
    "    # Create a mask to select features\n",
    "    mask = [bool(bit) for bit in individual]\n",
    "    X_selected = X[:, mask]\n",
    "    \n",
    "     # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train an XGBoost classifier\n",
    "    clf = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_preds = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate precision-recall AUC\n",
    "    y_score = clf.predict_proba(X_test)[:, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "    auc_score = auc(recall, precision)\n",
    "    \n",
    "    return auc_score,\n",
    "\n",
    "# Create a fitness function (maximize AUC)\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Genetic algorithm toolbox setup\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=30)  # 30 features\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluate, X=X_train, y=y_train)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Create the population\n",
    "population = toolbox.population(n=10)\n",
    "\n",
    "# Define the number of generations\n",
    "n_generations = 10\n",
    "\n",
    "# Run the genetic algorithm\n",
    "for gen in range(n_generations):\n",
    "    offspring = algorithms.varAnd(population, toolbox, cxpb=0.7, mutpb=0.2)\n",
    "    fits = toolbox.map(toolbox.evaluate, offspring)\n",
    "    \n",
    "    for fit, ind in zip(fits, offspring):\n",
    "        ind.fitness.values = fit\n",
    "    \n",
    "    population = toolbox.select(offspring, k=len(population))\n",
    "\n",
    "# Retrieve the best individual (features)\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "best_feature_mask = [bool(bit) for bit in best_individual]\n",
    "\n",
    "# Use the best feature subset for model training and testing\n",
    "X_selected = X_train[:, best_feature_mask]\n",
    "X_test_selected = X_test[:, best_feature_mask]\n",
    "\n",
    "clf = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_selected, y_train)\n",
    "y_preds = clf.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the final model with the best feature subset\n",
    "y_score = clf.predict_proba(X_test_selected)[:, 1]\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "auc_score = auc(recall, precision)\n",
    "\n",
    "# Print the AUC score of the final model\n",
    "print(f\"Final AUC Score: {auc_score}\")\n",
    "\n",
    "# The best_feature_mask contains the selected features for the final model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1116dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from deap import base, creator, tools, algorithms\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# Split the dataset into features (X) and the target variable (y)\n",
    "X = df.drop('Class', axis=1).values  # Assuming 'Class' is the target variable\n",
    "y = df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c676a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4efc13bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function (fitness function) using XGBoost\n",
    "def evaluate(individual, X, y):\n",
    "    # Create a mask to select features\n",
    "    mask = [bool(bit) for bit in individual]\n",
    "    X_selected = X[:, mask]\n",
    "    \n",
    "     # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train an XGBoost classifier\n",
    "    clf = xgb.XGBClassifier(n_estimators=10, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_preds = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate precision-recall AUC\n",
    "    y_score = clf.predict_proba(X_test)[:, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "    auc_score = auc(recall, precision)\n",
    "    \n",
    "     # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_preds)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_preds)\n",
    "    \n",
    "    return auc_score,accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aee1273a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    }
   ],
   "source": [
    "# Create a fitness function (maximize AUC)\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9cf7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genetic algorithm toolbox setup\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=30)  # 30 features\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluate, X=X_train, y=y_train)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6f647a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the population\n",
    "population = toolbox.population(n=10)\n",
    "\n",
    "# Define the number of generations\n",
    "n_generations = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "049f8d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the genetic algorithm\n",
    "for gen in range(n_generations):\n",
    "    offspring = algorithms.varAnd(population, toolbox, cxpb=0.7, mutpb=0.2)\n",
    "    fits = toolbox.map(toolbox.evaluate, offspring)\n",
    "    \n",
    "    for fit, ind in zip(fits, offspring):\n",
    "        ind.fitness.values = fit\n",
    "    \n",
    "    population = toolbox.select(offspring, k=len(population))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c11fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve the best individual (features)\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "best_feature_mask = [bool(bit) for bit in best_individual]\n",
    "\n",
    "# Use the best feature subset for model training and testing\n",
    "X_selected = X_train[:, best_feature_mask]\n",
    "X_test_selected = X_test[:, best_feature_mask]\n",
    "\n",
    "clf = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_selected, y_train)\n",
    "y_preds = clf.predict(X_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f8ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final model with the best feature subset\n",
    "y_score = clf.predict_proba(X_test_selected)[:, 1]\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "auc_score = auc(recall, precision)\n",
    "\n",
    "# Print the AUC score of the final model\n",
    "print(f\"Final AUC Score: {auc_score}\")\n",
    "\n",
    "# The best_feature_mask contains the selected features for the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c005e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final AUC Score: 0.8628450343680472\n"
     ]
    }
   ],
   "source": [
    "# Run the genetic algorithm\n",
    "for gen in range(n_generations):\n",
    "    offspring = algorithms.varAnd(population, toolbox, cxpb=0.7, mutpb=0.2)\n",
    "    fits = toolbox.map(toolbox.evaluate, offspring)\n",
    "    \n",
    "    for fit, ind in zip(fits, offspring):\n",
    "        ind.fitness.values = fit\n",
    "    \n",
    "    population = toolbox.select(offspring, k=len(population))\n",
    "\n",
    "# Retrieve the best individual (features)\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "best_feature_mask = [bool(bit) for bit in best_individual]\n",
    "\n",
    "# Use the best feature subset for model training and testing\n",
    "X_selected = X_train[:, best_feature_mask]\n",
    "X_test_selected = X_test[:, best_feature_mask]\n",
    "\n",
    "clf = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_selected, y_train)\n",
    "y_preds = clf.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the final model with the best feature subset\n",
    "y_score = clf.predict_proba(X_test_selected)[:, 1]\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "auc_score = auc(recall, precision)\n",
    "\n",
    "# Print the AUC score of the final model\n",
    "print(f\"Final AUC Score: {auc_score}\")\n",
    "\n",
    "# The best_feature_mask contains the selected features for the final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4eb448c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the final model with the best feature subset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_preds)\n\u001b[0;32m      3\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_test, y_preds)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the final model with the best feature subset\n",
    "accuracy = accuracy_score(y_test, y_preds)\n",
    "f1 = f1_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecc05af",
   "metadata": {},
   "source": [
    "# # In the provided code example, we are using the XGBoost classifier as the machine learning model to evaluate the feature subsets selected by the genetic algorithm. The genetic algorithm is used to perform feature selection, not to build a classifier. The genetic algorithm is used to select the most relevant features for the classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a09791",
   "metadata": {},
   "source": [
    "# we are setting up the genetic algorithm using the DEAP (Distributed Evolutionary Algorithms in Python) library. DEAP provides a framework for creating and customizing genetic algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1979a910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final AUC Score: 0.8180393825294777\n",
      "Final Accuracy Score: 0.9994908886626171\n",
      "Final F1 Score: 0.8379888268156425\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import random\n",
    "from deap import base, creator, tools, algorithms\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Split the dataset into features (X) and the target variable (y)\n",
    "X = df.drop('Class', axis=1).values  # Assuming 'Class' is the target variable\n",
    "y = df['Class'].values\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score \n",
    "\n",
    "# Define the evaluation function (fitness function) using XGBoost\n",
    "def evaluate(individual, X, y):\n",
    "    # Create a mask to select features\n",
    "    mask = [bool(bit) for bit in individual]\n",
    "    X_selected = X[:, mask]\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train an XGBoost classifier\n",
    "    clf = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_preds = clf.predict(X_test)\n",
    "\n",
    "    # Calculate precision-recall AUC\n",
    "    y_score = clf.predict_proba(X_test)[:, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "    auc_score = auc(recall, precision)\n",
    "\n",
    "    return auc_score,\n",
    "\n",
    "# Create a fitness function (maximize AUC)\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Genetic algorithm toolbox setup\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=30)  # 30 features\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluate, X=X_train, y=y_train)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Create the population\n",
    "population = toolbox.population(n=10)\n",
    "\n",
    "# Define the number of generations\n",
    "n_generations = 10\n",
    "\n",
    "# Run the genetic algorithm\n",
    "for gen in range(n_generations):\n",
    "    offspring = algorithms.varAnd(population, toolbox, cxpb=0.7, mutpb=0.2)\n",
    "    fits = toolbox.map(toolbox.evaluate, offspring)\n",
    "\n",
    "    for fit, ind in zip(fits, offspring):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    population = toolbox.select(offspring, k=len(population))\n",
    "\n",
    "# Retrieve the best individual (features)\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "\n",
    "# The best_feature_mask contains the selected features for the final model\n",
    "best_feature_mask = [bool(bit) for bit in best_individual]\n",
    "\n",
    "# Use the best feature subset for model training and testing\n",
    "X_selected = X_train[:, best_feature_mask]\n",
    "X_test_selected = X_test[:, best_feature_mask]\n",
    "\n",
    "clf = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_selected, y_train)\n",
    "y_preds = clf.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the final model with the best feature subset\n",
    "y_score = clf.predict_proba(X_test_selected)[:, 1]\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "auc_score = auc(recall, precision)\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy = accuracy_score(y_test, y_preds)\n",
    "f1 = f1_score(y_test, y_preds)\n",
    "\n",
    "# Print the AUC score, accuracy, and F1 score of the final model\n",
    "print(f\"Final AUC Score: {auc_score}\")\n",
    "print(f\"Final Accuracy Score: {accuracy}\")\n",
    "print(f\"Final F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd591170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
